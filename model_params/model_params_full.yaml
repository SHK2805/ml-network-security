models:
  Logistic Regression:
    model: LogisticRegression
    params:
      C: [0.01, 0.05, 0.1, 1, 10, 100]
      solver: ['newton-cg', 'lbfgs', 'liblinear']
  Decision Tree:
    model: DecisionTreeClassifier
    params:
      criterion: ['gini', 'entropy', 'log_loss']
      splitter: ['best', 'random']
      max_features: ['sqrt', 'log2']
      max_depth: [10, 20, 30, 40, 50]
  KNN:
    model: KNeighborsClassifier
    params:
      n_neighbors: [3, 5, 7, 9]
      weights: ['uniform', 'distance']
      algorithm: ['auto', 'ball_tree', 'kd_tree', 'brute']
  Random Forest:
    model: RandomForestClassifier
    params:
      n_estimators: [8, 16, 32, 50, 64, 100, 128, 200, 256]
      criterion: ['gini', 'entropy', 'log_loss']
      max_features: ['sqrt', 'log2']
      max_depth: [10, 20, 30]
  Gradient Boosting:
    model: GradientBoostingClassifier
    params:
      loss: ['deviance', 'exponential', 'log_loss']
      n_estimators: [8,16,32,50,64,100,128,200,256]
      learning_rate: [0.001, 0.01, 0.1, 0.2, 0.3, 0.5]
      subsample: [0.5, 0.6, 0.7, 0.75, 0.85, 0.9, 1.0]
      criterion: ['friedman_mse', 'mse', 'mae', 'squared_error']
      max_features: ['auto', 'sqrt', 'log2']
      max_depth: [3, 5, 7]
  AdaBoost:
    model: AdaBoostClassifier
    params:
      n_estimators: [8,16,32,50,64,100,128,200,256]
      learning_rate: [0.001, 0.01, 0.1, 0.2, 0.3]